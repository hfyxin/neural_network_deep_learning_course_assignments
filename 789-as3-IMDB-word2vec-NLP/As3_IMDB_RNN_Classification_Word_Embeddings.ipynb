{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "As3. IMDB RNN Classification, Word Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKGXPQMVScHX",
        "colab_type": "text"
      },
      "source": [
        "# CHEMENG/MECHENG 789 Assignment 3\n",
        "\n",
        "Elliot (Yixin) Huangfu\n",
        "\n",
        "In this assignment the objective is to reproduce the Sentiment Classification example as presented here:\n",
        "\n",
        "https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456\n",
        "\n",
        "You need to upload the data to your google drive and develop a jupyeter notebook using colab. Your deliverables are\n",
        "\n",
        "- an executable code\n",
        " - **The blog example is fully replicated, using tensorflow 2.2.0-rc1 in Colab.**\n",
        "- some example that you make for testing your model (predict)\n",
        " - **Customized testing case is in the *Test model* section.**\n",
        "- how to improve the model better? \n",
        " - **Hyper-parameter tuning, including increasing RNN layer & size, and using LSTM may help improve the result.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KArYrkcPTUR0",
        "colab_type": "code",
        "outputId": "69e918f3-74ba-4d23-fd05-24bb8aa095cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# for colab, set tf version\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0-rc2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akP5C7vQP-WP",
        "colab_type": "text"
      },
      "source": [
        "download the IMDB movie dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xigy8IvesKQ",
        "colab_type": "code",
        "outputId": "f577a306-c4da-4774-dd01-b2aa2aae8f29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import requests\n",
        "url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "myfile = requests.get(url)\n",
        "open('./aclImdb_v1.tar.gz', 'wb').write(myfile.content)\n",
        "\n",
        "print('file downloaded:', './aclImdb_v1.tar.gz')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file downloaded: ./aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJlw9WKugusU",
        "colab_type": "text"
      },
      "source": [
        "## Load IMDB dataset\n",
        "\n",
        "Make sure the original data file (tar.gz) exists in current folder. Extract file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9im5nobe4y-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile \n",
        "filepath = './aclImdb_v1.tar.gz'\n",
        "with tarfile.open(filepath,'r') as tar_ref:\n",
        "    tar_ref.extractall(\"./\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h6XyFeajDK1",
        "colab_type": "text"
      },
      "source": [
        "Search through folder and obtain all file names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ1RIj9tjBo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "folder = './aclImdb'\n",
        "train_files = {}   # contain filenames\n",
        "test_files = {}   # contain filenames\n",
        "train_files['pos'] = os.listdir(folder + '/train/pos')\n",
        "train_files['neg'] = os.listdir(folder + '/train/neg')\n",
        "test_files['pos'] = os.listdir(folder + '/test/pos')\n",
        "test_files['neg'] = os.listdir(folder + '/test/neg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXAmgeILowsa",
        "colab_type": "text"
      },
      "source": [
        "Read every .txt file and keep in DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KERrpwNPhjyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_text = []\n",
        "train_label = []\n",
        "test_text = []\n",
        "test_label = []\n",
        "\n",
        "# loop through each filename\n",
        "for label, filenames in train_files.items():\n",
        "    current_folder = os.path.join('./aclImdb/train/',label)\n",
        "    for filename in filenames:\n",
        "        full_fn = os.path.join(current_folder, filename)\n",
        "        with open(full_fn, 'r') as f:\n",
        "            train_text.append(f.read())\n",
        "        if label == 'pos': train_label.append(1)\n",
        "        elif label == 'neg': train_label.append(0)\n",
        "        else: print('unhandled label:',label)\n",
        "\n",
        "# test dataset\n",
        "for label, filenames in test_files.items():\n",
        "    current_folder = os.path.join('./aclImdb/test/',label)\n",
        "    for filename in filenames:\n",
        "        full_fn = os.path.join(current_folder, filename)\n",
        "        with open(full_fn, 'r') as f:\n",
        "            test_text.append(f.read())\n",
        "        if label == 'pos': test_label.append(1)\n",
        "        elif label == 'neg': test_label.append(0)\n",
        "        else: print('unhandled label:',label)\n",
        "\n",
        "# convert to dataframes\n",
        "train_df = pd.DataFrame({'review':train_text, 'sentiment':train_label})\n",
        "test_df = pd.DataFrame({'review':test_text, 'sentiment':test_label})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc9JAxeCp9Wl",
        "colab_type": "text"
      },
      "source": [
        "Examine the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTWqCm3Bojq0",
        "colab_type": "code",
        "outputId": "8fdfbb98-29cd-404f-e822-398c6bfaf9c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "print(train_df.head())\n",
        "print(test_df.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              review  sentiment\n",
            "0  This movie makes a statement about Joseph Smit...          1\n",
            "1  Maybe one of the most entertaining Ninja-movie...          1\n",
            "2  ever watched. It deals so gently and subtly no...          1\n",
            "3  To a certain extent, I actually liked this fil...          1\n",
            "4  ** Warning - this post may contain spoilers **...          1\n",
            "                                              review  sentiment\n",
            "0  What can I say about this film other than the ...          1\n",
            "1  This is my all time favorite Looney Tunes cart...          1\n",
            "2  \"Mr. Bug Goes To Town\" was the last major achi...          1\n",
            "3  One of the more interesting films I've seen. L...          1\n",
            "4  Every once in a while a film comes along with ...          1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FLkJSO12iSN",
        "colab_type": "text"
      },
      "source": [
        "Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rB3Rozb2gyI",
        "colab_type": "code",
        "outputId": "a203dd2b-ae8a-48e8-f17c-af84da9ea5a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x_train = train_df['review'].values\n",
        "y_train = train_df['sentiment'].values\n",
        "x_test = test_df['review'].values\n",
        "y_test = test_df['sentiment'].values\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000,) (25000,)\n",
            "(25000,) (25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCCimOlJ2-Mo",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8B1_Anh4Ho-",
        "colab_type": "code",
        "outputId": "035f543f-c621-4b4e-8e85-c0082a12e8cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# fit tokenizer\n",
        "tokenizer_obj = Tokenizer()\n",
        "all_reviews = np.concatenate((x_train, x_test), axis=0)\n",
        "tokenizer_obj.fit_on_texts(all_reviews)\n",
        "\n",
        "# pad sequences\n",
        "max_length = max([len(s.split()) for s in all_reviews])\n",
        "\n",
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer_obj.word_index) + 1\n",
        "\n",
        "# create tokenized dataset\n",
        "x_train_tokens = tokenizer_obj.texts_to_sequences(x_train)\n",
        "x_test_tokens = tokenizer_obj.texts_to_sequences(x_test)\n",
        "\n",
        "# pad sequence\n",
        "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_length)\n",
        "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_length)\n",
        "\n",
        "print(x_train_pad.shape, x_test_pad.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 2470) (25000, 2470)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzCxH0j3WYEr",
        "colab_type": "text"
      },
      "source": [
        "## Simple model: word embedding + RNN\n",
        "\n",
        "The following settings are in consistent with the blog example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE_J39evbtRv",
        "colab_type": "code",
        "outputId": "6227aa04-11fc-4d80-c545-91859f2ab46e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# check the input parameters:\n",
        "print('vocab_size:', vocab_size)\n",
        "print('max_length:', max_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size: 124253\n",
            "max_length: 2470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcjfemvMWWb7",
        "colab_type": "code",
        "outputId": "13461677-10f8-44be-d730-a52cd469ba61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# build a model\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length),\n",
        "    tf.keras.layers.GRU(units=32, dropout=0.2, recurrent_dropout=0.2),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu471ysc45IO",
        "colab_type": "code",
        "outputId": "24b7f5ff-6226-42e4-8fb8-26b4ccd2a596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 2470, 100)         12425300  \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 32)                12864     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 12,438,197\n",
            "Trainable params: 12,438,197\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5c-_VLJ5GHB",
        "colab_type": "text"
      },
      "source": [
        "Training. since the GRU with recurrent_dropout does not meet cuDNN kernel criteria, the training is very slow.\n",
        "\n",
        "Since the trainable parameter is very large, the model would overfit badly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdgR-QlL5FQU",
        "colab_type": "code",
        "outputId": "932bedfb-3463-4bfb-991f-07140d052a5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "model.fit(\n",
        "    x_train_pad, y_train,\n",
        "    batch_size=128, epochs=5,\n",
        "    validation_data=(x_test_pad,y_test), verbose=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "196/196 [==============================] - 2552s 13s/step - loss: 0.4799 - accuracy: 0.7518 - val_loss: 0.3478 - val_accuracy: 0.8558\n",
            "Epoch 2/5\n",
            "196/196 [==============================] - 2506s 13s/step - loss: 0.2241 - accuracy: 0.9162 - val_loss: 0.3424 - val_accuracy: 0.8554\n",
            "Epoch 3/5\n",
            "196/196 [==============================] - 2470s 13s/step - loss: 0.1232 - accuracy: 0.9584 - val_loss: 0.3688 - val_accuracy: 0.8660\n",
            "Epoch 4/5\n",
            "196/196 [==============================] - 2506s 13s/step - loss: 0.0840 - accuracy: 0.9730 - val_loss: 0.4488 - val_accuracy: 0.8530\n",
            "Epoch 5/5\n",
            " 73/196 [==========>...................] - ETA: 23:38 - loss: 0.0514 - accuracy: 0.9833"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QzY9ejddM4K",
        "colab_type": "text"
      },
      "source": [
        "### Test model (with customized examples)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZRSIbuXdP9g",
        "colab_type": "code",
        "outputId": "246104c5-388d-4eba-d3ef-653a36d0d141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "test_samples = [\n",
        "    \"This movie really sucks! Can I get my money back please?\",\n",
        "    \"Not a good movie!\",\n",
        "    \"This movie is fantastic! I really like it because it is so good.\",\n",
        "    ]\n",
        "test_samples_tokens = tokenizer_obj.texts_to_sequences(test_samples)\n",
        "test_samples_pad = pad_sequences(test_samples_tokens, maxlen=max_length)\n",
        "\n",
        "# predict\n",
        "model.predict(test_samples_pad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.13415487],\n",
              "       [0.5118695 ],\n",
              "       [0.83251464]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfOgu70LZ-fN",
        "colab_type": "text"
      },
      "source": [
        "## word2vec Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N1IsYezu1XZ",
        "colab_type": "code",
        "outputId": "12781777-43f2-4f7d-ab0d-7b328dc30c7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        }
      },
      "source": [
        "# download some data for nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A58z2UQ6wgOX",
        "colab_type": "text"
      },
      "source": [
        "Manually process the review text:\n",
        "- remove all punctuation\n",
        "- remove all non-alphabetic characters\n",
        "- remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jrI1hpnZ2bt",
        "colab_type": "code",
        "outputId": "db67f2f6-f614-41c5-8e91-9d69d8b31b58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "all_reviews_words = []\n",
        "for line in all_reviews.tolist():\n",
        "    # conver each review into list of words\n",
        "    words = word_tokenize(line)\n",
        "    # covert to lowercase\n",
        "    words = [w.lower() for w in words]\n",
        "    # remove punctuation from word (not from list)\n",
        "    punct_dict = str.maketrans('','',string.punctuation)\n",
        "    words = [w.translate(punct_dict) for w in words]\n",
        "    # remove words that are not alphabetic\n",
        "    words = [w for w in words if w.isalpha()]\n",
        "    # remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "\n",
        "    all_reviews_words.append(words)\n",
        "\n",
        "len(all_reviews_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WLnvuguxPTq",
        "colab_type": "text"
      },
      "source": [
        "Examine the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXfZlMb1v-hi",
        "colab_type": "code",
        "outputId": "93979acf-2418-4a62-bb1a-6553f0c8b138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "i = 1\n",
        "print('Original text:\\n', x_train[i])\n",
        "print('Processed text:\\n', ' '.join(all_reviews_words[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original text:\n",
            " Maybe one of the most entertaining Ninja-movies ever made. A hard-hitting action movie with lots of gore and slow motion (eehaaa!). Made in ´83 and still the greatest swedish action movie made so far! And we can hardly wait to see the upcoming sequel, Ninja mission 2000 - The legacy of Markov!\n",
            "Processed text:\n",
            " maybe one entertaining ninjamovies ever made hardhitting action movie lots gore slow motion eehaaa made still greatest swedish action movie made far hardly wait see upcoming sequel ninja mission legacy markov\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvPCJ7_3yQ-u",
        "colab_type": "text"
      },
      "source": [
        "Train word embedding using gensim."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP5cfqhOxvrv",
        "colab_type": "code",
        "outputId": "ff468e11-a075-432a-e07f-c8ff507b7291",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import gensim\n",
        "\n",
        "# train word2vec model\n",
        "model = gensim.models.Word2Vec(sentences=all_reviews_words, size=EMBEDDING_DIM, window=5, workers=4, min_count=1)\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.word2vec.Word2Vec at 0x7f5933917860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZFltSd-zzsW",
        "colab_type": "text"
      },
      "source": [
        "Examine the word embeding model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdnHpOHOv-d5",
        "colab_type": "code",
        "outputId": "74a729f6-e686-4041-9f37-16dd316ba0d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "vocab = list(model.wv.vocab.keys())\n",
        "print('vocabulary size:', len(vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocabulary size: 134156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjYH-Rxuzf6-",
        "colab_type": "code",
        "outputId": "365ffd91-b793-443b-d22b-f3c4d3fdd3a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "source": [
        "# similar words\n",
        "model.wv.most_similar('horrible')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('terrible', 0.9212156534194946),\n",
              " ('awful', 0.8781707882881165),\n",
              " ('pathetic', 0.7718660831451416),\n",
              " ('atrocious', 0.7631697058677673),\n",
              " ('horrendous', 0.7616218328475952),\n",
              " ('dreadful', 0.7500942349433899),\n",
              " ('sucks', 0.7495220303535461),\n",
              " ('horrid', 0.7411223649978638),\n",
              " ('lousy', 0.7325442433357239),\n",
              " ('bad', 0.7231553792953491)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E0hr_-I0ccm",
        "colab_type": "code",
        "outputId": "26b6d7ea-5e80-4707-9187-b2828841b4d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "# math\n",
        "model.wv.most_similar_cosmul(positive=['woman','king'], negative=['man'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('princess', 0.8775322437286377),\n",
              " ('romeo', 0.8715057969093323),\n",
              " ('bride', 0.8615204691886902),\n",
              " ('juliet', 0.856743335723877),\n",
              " ('tearle', 0.8564087152481079),\n",
              " ('queen', 0.8427306413650513),\n",
              " ('changxin', 0.8397428393363953),\n",
              " ('ciarán', 0.8384663462638855),\n",
              " ('crimecop', 0.8383185863494873),\n",
              " ('jetée', 0.8341542482376099)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKCgo_W11mh9",
        "colab_type": "code",
        "outputId": "d71f6312-d638-4716-8440-31ab0d487407",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# find odd word\n",
        "model.wv.doesnt_match('woman king queen movie'.split())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'movie'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8scOjkVj2BbN",
        "colab_type": "code",
        "outputId": "779ba591-c1de-4ca3-9d4f-19b2c45cc754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# save model\n",
        "w2v_fn = 'imdb_embedding_word2vec.txt'\n",
        "model.wv.save_word2vec_format(w2v_fn, binary=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7gvUM1T2p0Z",
        "colab_type": "text"
      },
      "source": [
        "## Apply word embedding\n",
        "\n",
        "the word embedding obtained previously is word to vector, tokenization is skipped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzxgC8-W2pEr",
        "colab_type": "code",
        "outputId": "e979eaac-ae6e-4fe3-96e4-2d9940090f51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "import os\n",
        "embeddings = {}\n",
        "print(w2v_fn)  # should be 'imdb_embedding_word2vec.txt'\n",
        "\n",
        "with open(os.path.join('.', w2v_fn), 'r') as f:\n",
        "    # skip the first line\n",
        "    print('vocab_size, embedding_dims:', f.readline())\n",
        "    # read every line and store as numpy array\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        embeddings[word] = np.array([float(v) for v in values[1:]])\n",
        "\n",
        "len(embeddings)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "imdb_embedding_word2vec.txt\n",
            "vocab_size, embedding_dims: 134156 100\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "134156"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhpHoEuV8qTw",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing - for manually processed texts\n",
        "To play fair, use the train/test split from the original dataset. The blog tutorial uses a 0.8/0.2 split which is different from original."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUpYFhCA9BAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create new dataset based on the processed text sequences\n",
        "x_train_new = np.array([' '.join(words) for words in all_reviews_words[0:25000]])\n",
        "x_test_new = np.array([' '.join(words) for words in all_reviews_words[25000:]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gON4TEPuAKwf",
        "colab_type": "text"
      },
      "source": [
        "Check the difference btw original and processed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRbIxsWi9iLo",
        "colab_type": "code",
        "outputId": "b934e623-1f85-4116-8f08-386fcf1521e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "i = 18\n",
        "print('Train data comparison:', y_train[i])\n",
        "print(x_train[i])\n",
        "print(x_train_new[i])\n",
        "\n",
        "print('\\nTest data comparison:', y_test[i])\n",
        "print(x_test[i])\n",
        "print(x_test_new[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data comparison: 1\n",
            "A very good wartime movie showing the effects of war on a hometown boy who looses his eyesight on Guadalcanal and must come home and re-adjust himself with the help of family and friends. An excellent cast of actor's helps make this movie very entertaining. Eleanor Parker's role as the girlfriend was worthy of an Oscar nomination. She has such an innocence to her in this movie. Ann Doran role was equally satisfying as was all of her small supporting roles. I especially like the hometown aura of pre-war Phildelphia. The hunting scene is very good. Of course the war scene on Guadalcanal truly showed the horror faced by our soldiers during this epic battle. A well deserving film and one that should not be forgotten\n",
            "good wartime movie showing effects war hometown boy looses eyesight guadalcanal must come home readjust help family friends excellent cast actor helps make movie entertaining eleanor parker role girlfriend worthy oscar nomination innocence movie ann doran role equally satisfying small supporting roles especially like hometown aura prewar phildelphia hunting scene good course war scene guadalcanal truly showed horror faced soldiers epic battle well deserving film one forgotten\n",
            "\n",
            "Test data comparison: 1\n",
            "This was an excellent movie! I saw this at the Karlovy Vary IFF in the Czech Republic, and it won an award there. This is the first film I've ever seen from Jan (the director), and I was impressed. It's a great story about love and family. The movie has a great balance of comedy, romance, drama, and suspense all in one. I will not give away any of the plot, but this is a well-made film, and I would watch it again if I had the chance! The cinematography/editing is great, the film simply flows, and the characters are warm, and they are the kind that one can relate to. I hope you can enjoy this film as I did. If anyone knows where I can find this in the United States, or if they plan on releasing it on DVD anytime soon, please let me know!!\n",
            "excellent movie saw karlovy vary iff czech republic award first film ever seen jan director impressed great story love family movie great balance comedy romance drama suspense one give away plot wellmade film would watch chance cinematographyediting great film simply flows characters warm kind one relate hope enjoy film anyone knows find united states plan releasing dvd anytime soon please let know\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0LTZVqeAc3J",
        "colab_type": "text"
      },
      "source": [
        "preprocessing same as before"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3iSQTCQ30GU",
        "colab_type": "code",
        "outputId": "7704d1a0-9647-4f4e-fece-c52198e01704",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "# fit tokenizer\n",
        "tokenizer_obj = Tokenizer()\n",
        "tokenizer_obj.fit_on_texts(np.concatenate((x_train_new, x_test_new), axis=0))\n",
        "\n",
        "# pad sequences\n",
        "max_length = max([len(w) for w in all_reviews_words])\n",
        "\n",
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer_obj.word_index) + 1\n",
        "\n",
        "# create tokenized dataset\n",
        "x_train_tokens = tokenizer_obj.texts_to_sequences(x_train_new)\n",
        "x_test_tokens = tokenizer_obj.texts_to_sequences(x_test_new)\n",
        "\n",
        "# pad sequence\n",
        "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_length)\n",
        "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_length)\n",
        "\n",
        "print('vocab_size:', vocab_size)\n",
        "print('max_length:', max_length)\n",
        "print('shape of train / test dataset:')\n",
        "print(x_train_pad.shape, x_test_pad.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size: 134157\n",
            "max_length: 1440\n",
            "shape of train / test dataset:\n",
            "(25000, 1440) (25000, 1440)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SscbacuZC_zW",
        "colab_type": "text"
      },
      "source": [
        "So far we have two dicts: word - token and word - embedding. Now obtain the token - embedding dict (matrix)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k32Lq9waC_Xm",
        "colab_type": "code",
        "outputId": "40862fdb-f541-42b1-cd50-0c52d99d3f6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# obtain token - embedding matrix\n",
        "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "\n",
        "for word, i in tokenizer_obj.word_index.items():\n",
        "    embedding_matrix[i] = embeddings[word]\n",
        "\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(134157, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWRIZvKXJHe7",
        "colab_type": "text"
      },
      "source": [
        "## RNN model: with pre-trained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOb_3Duf2Mr8",
        "colab_type": "code",
        "outputId": "656f7a36-7e91-4b91-ac98-73c0bd03dcc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "# check the input parameters:\n",
        "print('vocab_size:', vocab_size)\n",
        "print('max_length:', max_length)\n",
        "print('embedding_dims:', EMBEDDING_DIM)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size: 134157\n",
            "max_length: 1440\n",
            "embedding_dims: 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grSZ5e7vB3v_",
        "colab_type": "code",
        "outputId": "bd0d9826-8643-4ee3-f768-2b45231f427d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# build a model\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(\n",
        "        vocab_size, EMBEDDING_DIM,\n",
        "        embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "        input_length=max_length,\n",
        "        trainable=False),\n",
        "    tf.keras.layers.GRU(units=32, dropout=0.2, recurrent_dropout=0.2),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "473v1ZrpJsRd",
        "colab_type": "code",
        "outputId": "4fad5e11-b8d9-406c-e505-c6b5f88ce97a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "# trainable parameters is significantly fewer\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 1440, 100)         13415700  \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 32)                12864     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 13,428,597\n",
            "Trainable params: 12,897\n",
            "Non-trainable params: 13,415,700\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPZxPqJbJ_9R",
        "colab_type": "code",
        "outputId": "4e045329-98e2-4b40-bc15-67be6a59d013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "source": [
        "model.fit(\n",
        "    x_train_pad, y_train,\n",
        "    batch_size=128, epochs=10,\n",
        "    validation_data=(x_test_pad,y_test), verbose=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "196/196 [==============================] - 1443s 7s/step - loss: 0.5905 - accuracy: 0.6767 - val_loss: 0.4728 - val_accuracy: 0.7791\n",
            "Epoch 2/10\n",
            "196/196 [==============================] - 1385s 7s/step - loss: 0.4233 - accuracy: 0.8126 - val_loss: 0.3655 - val_accuracy: 0.8428\n",
            "Epoch 3/10\n",
            "196/196 [==============================] - 1358s 7s/step - loss: 0.3697 - accuracy: 0.8418 - val_loss: 0.3321 - val_accuracy: 0.8585\n",
            "Epoch 4/10\n",
            "196/196 [==============================] - 1357s 7s/step - loss: 0.3392 - accuracy: 0.8561 - val_loss: 0.3434 - val_accuracy: 0.8536\n",
            "Epoch 5/10\n",
            "196/196 [==============================] - 1371s 7s/step - loss: 0.3228 - accuracy: 0.8648 - val_loss: 0.3021 - val_accuracy: 0.8718\n",
            "Epoch 6/10\n",
            "196/196 [==============================] - 1362s 7s/step - loss: 0.3137 - accuracy: 0.8676 - val_loss: 0.3049 - val_accuracy: 0.8701\n",
            "Epoch 7/10\n",
            "196/196 [==============================] - 1380s 7s/step - loss: 0.3045 - accuracy: 0.8717 - val_loss: 0.2921 - val_accuracy: 0.8793\n",
            "Epoch 8/10\n",
            "196/196 [==============================] - 1384s 7s/step - loss: 0.2999 - accuracy: 0.8741 - val_loss: 0.2873 - val_accuracy: 0.8801\n",
            "Epoch 9/10\n",
            "196/196 [==============================] - 1402s 7s/step - loss: 0.2950 - accuracy: 0.8759 - val_loss: 0.2816 - val_accuracy: 0.8808\n",
            "Epoch 10/10\n",
            "196/196 [==============================] - 1328s 7s/step - loss: 0.2900 - accuracy: 0.8788 - val_loss: 0.2805 - val_accuracy: 0.8828\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5a1f505ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvapVCRJL_ea",
        "colab_type": "text"
      },
      "source": [
        "### Test model (with customized examples)\n",
        "\n",
        "This model is able to tell that \"Not a good movie!\" is a negative review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpBBDgKoL_HZ",
        "colab_type": "code",
        "outputId": "0cfb0080-01b7-4fdc-95a1-8ccb74de5040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "# test some samples\n",
        "test_samples = [\n",
        "    \"This movie really sucks! Can I get my money back please?\",\n",
        "    \"Not a good movie!\",\n",
        "    \"This movie is fantastic! I really like it because it is so good.\",\n",
        "    ]\n",
        "\n",
        "test_samples_tokens = tokenizer_obj.texts_to_sequences(test_samples)\n",
        "test_samples_pad = pad_sequences(test_samples_tokens, maxlen=max_length)\n",
        "\n",
        "# predict\n",
        "model.predict(test_samples_pad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.13060334],\n",
              "       [0.5897073 ],\n",
              "       [0.91971165]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3uXzO0uMrdv",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "Train from scratch, test accuracy: ~86%\n",
        "\n",
        "Pre-trained word embedding, test accuracy: ~88%\n",
        "\n",
        "With pre-trained embedding, the training is much faster and less prone to overfitting.\n",
        "\n"
      ]
    }
  ]
}