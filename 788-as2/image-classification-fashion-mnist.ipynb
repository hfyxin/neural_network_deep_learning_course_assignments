{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYysdyb-CaWM"
   },
   "source": [
    "## Image classification\n",
    "\n",
    "Yixin Huangfu\n",
    "\n",
    "This assignment is about trains a neural network model to classify images of clothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FbVhjPpzn6BM"
   },
   "source": [
    "In this assignment we are performing a classification task.\n",
    "- elaborate on the strategy that you use for design and optimization\n",
    "- when you decide to stop the training\n",
    "- how you add the Tensorboard to this assignment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jL3OqFKZ9dFg"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dzLKpmZICaWN"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yR0EdgrLCaWR"
   },
   "source": [
    "## Import the Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DLdCchMdCaWQ"
   },
   "source": [
    "We use the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains 70,000 grayscale images in 10 categories:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>\n",
    "\n",
    "Fashion MNIST is intended as a drop-in replacement for the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset—\n",
    "Import and load the Fashion MNIST data directly from keras (it is part of from keras.datasets ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7MqDQO0KCaWS"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9FDsUlxCaWW"
   },
   "source": [
    "Loading the dataset returns four NumPy arrays:\n",
    "\n",
    "* The `train_images` and `train_labels` arrays are the *training set*—the data the model uses to learn.\n",
    "* The model is tested against the *test set*, the `test_images`, and `test_labels` arrays.\n",
    "\n",
    "The images are 28x28 NumPy arrays, with pixel values ranging from 0 to 255. The *labels* are an array of integers, ranging from 0 to 9. These correspond to the *class* of clothing the image represents:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Label</th>\n",
    "    <th>Class</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>T-shirt/top</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Trouser</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>2</td>\n",
    "    <td>Pullover</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>3</td>\n",
    "    <td>Dress</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>4</td>\n",
    "    <td>Coat</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>5</td>\n",
    "    <td>Sandal</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>6</td>\n",
    "    <td>Shirt</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>7</td>\n",
    "    <td>Sneaker</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>8</td>\n",
    "    <td>Bag</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>9</td>\n",
    "    <td>Ankle boot</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Each image is mapped to a single label. Since the *class names* are not included with the dataset, store them here to use later when plotting the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IjnLH5S2CaWx"
   },
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Brm0b_KACaWX"
   },
   "source": [
    "## Explore the data\n",
    "\n",
    "Let's explore the format of the dataset before training the model. The following shows there are 60,000 images in the training set, with each image represented as 28 x 28 pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zW5k_xz1CaWX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cIAcvQqMCaWf"
   },
   "source": [
    "Likewise, there are 60,000 labels in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TRFYHB2mCaWb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YSlYxFuRCaWk"
   },
   "source": [
    "Each label is an integer between 0 and 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XKnCTHz4CaWg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TMPI88iZpO2T"
   },
   "source": [
    "There are 10,000 images in the test set. Again, each image is represented as 28 x 28 pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2KFnYlcwCaWl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rd0A0Iu0CaWq"
   },
   "source": [
    "And the test set contains 10,000 images labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJmPr5-ACaWn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ES6uQoLKCaWr"
   },
   "source": [
    "## Preprocess the data\n",
    "\n",
    "The data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m4VEw8Ud9Quh"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_images[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wz7l27Lz9S1P"
   },
   "source": [
    "Scale these values to a range of 0 to 1 before feeding them to the neural network model. To do so, divide the values by 255. It's important that the *training set* and the *testing set* be preprocessed in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bW5WzIPlCaWv"
   },
   "outputs": [],
   "source": [
    "train_images = train_images / 255\n",
    "\n",
    "test_images = test_images / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "59veuiEZCaW4"
   },
   "source": [
    "## Build the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gxg1XGm0eOBy"
   },
   "source": [
    "### Set up the layers\n",
    "\n",
    "The basic building block of a neural network is the *layer*. The first layer in this network should convert the image to a vector (784 pixels) you may find the following layer useful: tf.keras.layers.Flatten and tf.keras.layers.Dense as well as softmax layer 10 clsses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gut8A_7rCaW6"
   },
   "source": [
    "\n",
    "\n",
    "### Compile the model\n",
    "\n",
    "Before the model is ready for training, it needs a few more settings. These are added during the model's *compile* step:\n",
    "\n",
    "* *Loss function* —This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n",
    "* *Optimizer* —This is how the model is updated based on the data it sees and its loss function.\n",
    "* *Metrics* —Used to monitor the training and testing steps. The following example uses *accuracy*, the fraction of the images that are correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lhan11blCaW7"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKF6uW-BCaW-"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Training the neural network model requires the following steps:\n",
    "\n",
    "1. Feed the training data to the model. In this example, the training data is in the `train_images` and `train_labels` arrays.\n",
    "2. The model learns to associate images and labels.\n",
    "3. You ask the model to make predictions about a test set—in this example, the `test_images` array.\n",
    "4. Verify that the predictions match the labels from the `test_labels` array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z4P4zIV7E28Z"
   },
   "source": [
    "### Feed the model\n",
    "\n",
    "To start training,  call the `model.fit` method—so called because it \"fits\" the model to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xvwvpA64CaW_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4884 - accuracy: 0.8250\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.3642 - accuracy: 0.8658\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.3296 - accuracy: 0.8782\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.3077 - accuracy: 0.8866\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.2900 - accuracy: 0.8923\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.2745 - accuracy: 0.8974\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.2651 - accuracy: 0.9008\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.2520 - accuracy: 0.9049\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.2424 - accuracy: 0.9081\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.2363 - accuracy: 0.9098\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.2243 - accuracy: 0.9144\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.2187 - accuracy: 0.9164\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.2135 - accuracy: 0.9181\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.2073 - accuracy: 0.9208\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.1996 - accuracy: 0.9238\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.1933 - accuracy: 0.9269\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.1898 - accuracy: 0.9268\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.1837 - accuracy: 0.9294s - los\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.1758 - accuracy: 0.9328\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.1748 - accuracy: 0.9326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a1a0630198>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W3ZVOhugCaXA"
   },
   "source": [
    "As the model trains, the loss and accuracy metrics are displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wCpr6DGyE28h"
   },
   "source": [
    "### Evaluate accuracy\n",
    "\n",
    "Next, compare how the model performs on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VflXLEeECaXC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/1 - 0s - loss: 0.3656 - accuracy: 0.8899\n",
      "\n",
      "Test accuracy: 0.8899\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWfgsmVXCaXG"
   },
   "source": [
    "Elaborate on the overfitting/ underfitting and how to fix it:\n",
    "\n",
    "**Answer**\n",
    "\n",
    "The original training result gives training accuracy of 93%, while test accuracy is 89%. This is an overfitting issue. With dataset being fixed, there are two ways to overcome this.\n",
    "\n",
    "- Use regularization\n",
    "- Use a better model\n",
    "\n",
    "I tried to add l2 regularization to the Dense layer, but it doesn't help much. However, I successfully reduced the overfitting with another technique called Dropout, as shown by ```model2``` below. Now the train acc is 90.7% vs test acc 88.6%, difference is 2%.\n",
    "\n",
    "Using Convolutional Neural Network with Dropout also help reduce overfitting. The ```model3``` below shows train acc 92.6% vs test acc 91.4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ODch-OFCaW4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.6304 - accuracy: 0.7941\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4602 - accuracy: 0.8464\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4162 - accuracy: 0.8604\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.3899 - accuracy: 0.8648\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.3721 - accuracy: 0.8724\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.3591 - accuracy: 0.8766\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.3520 - accuracy: 0.8805\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.3415 - accuracy: 0.8822\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.3357 - accuracy: 0.8846\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.3283 - accuracy: 0.8856\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.3227 - accuracy: 0.8878\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.3157 - accuracy: 0.8899\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.3155 - accuracy: 0.8910\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.3093 - accuracy: 0.8932\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.3076 - accuracy: 0.8924\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.2985 - accuracy: 0.8961\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.2990 - accuracy: 0.8965\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.2954 - accuracy: 0.8976\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.2903 - accuracy: 0.8998\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.2872 - accuracy: 0.9013\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.2873 - accuracy: 0.9008\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.2829 - accuracy: 0.9007\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.2790 - accuracy: 0.9036\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.2778 - accuracy: 0.9033\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.2742 - accuracy: 0.9066s - loss: 0.2749 - ac - ETA: 0s - loss: 0.2762 \n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.2689 - accuracy: 0.9076\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.2722 - accuracy: 0.9051s -\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.2669 - accuracy: 0.9075\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.2702 - accuracy: 0.9061\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.2638 - accuracy: 0.9075\n",
      "10000/1 - 0s - loss: 0.2284 - accuracy: 0.8861\n",
      "\n",
      "Test accuracy: 0.8861\n"
     ]
    }
   ],
   "source": [
    "# Adding regularization and dropout to reduce overfitting\n",
    "model2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.fit(train_images, train_labels, epochs=30)\n",
    "\n",
    "test_loss, test_acc = model2.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 0.5424 - accuracy: 0.8023\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.3820 - accuracy: 0.8592\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.3397 - accuracy: 0.8752\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.3131 - accuracy: 0.8840\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.2904 - accuracy: 0.8926\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.2762 - accuracy: 0.8967s - loss: 0.2762 - accu\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.2619 - accuracy: 0.9028\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.2520 - accuracy: 0.9067\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.2423 - accuracy: 0.9098\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.2385 - accuracy: 0.9112\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.2316 - accuracy: 0.9134\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.2271 - accuracy: 0.9149\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.2210 - accuracy: 0.9160\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.2151 - accuracy: 0.9194\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.2135 - accuracy: 0.9196\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.2079 - accuracy: 0.9230\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.2040 - accuracy: 0.9236\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.2015 - accuracy: 0.9231\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.1981 - accuracy: 0.9246\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.1951 - accuracy: 0.9268\n",
      "10000/1 - 1s - loss: 0.2153 - accuracy: 0.9142\n",
      "\n",
      "Test accuracy: 0.9142\n"
     ]
    }
   ],
   "source": [
    "# Using CNN and Dropout\n",
    "model3 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, 3, activation='relu', input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "model3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model3.fit(train_images.reshape(train_images.shape + (1,)), train_labels, epochs=20)\n",
    "\n",
    "\n",
    "test_loss, test_acc = model3.evaluate(test_images.reshape(test_images.shape + (1,)), test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v-PyD1SYE28q"
   },
   "source": [
    "### Make predictions\n",
    "\n",
    "With the model trained, you can use it to make predictions about some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gl91RPhdCaXI"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x9Kk1voUCaXJ"
   },
   "source": [
    "Here, the model has predicted the label for each image in the testing set. Let's take a look at the first prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3DmJEUinCaXK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1144025e-07, 2.2921041e-09, 2.6876382e-06, 1.0987911e-12,\n",
       "       2.7849557e-08, 5.1741297e-03, 8.5812991e-07, 1.5629863e-02,\n",
       "       4.9265689e-04, 9.7869933e-01], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-hw1hgeSCaXN"
   },
   "source": [
    "A prediction is an array of 10 numbers. They represent the model's \"confidence\" that the image corresponds to each of the 10 different articles of clothing. You can see which label has the highest confidence value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qsqenuPnCaXO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E51yS7iCCaXO"
   },
   "source": [
    "So, the model is most confident that this image is an ankle boot, or `class_names[9]`. Examining the test label shows that this classification is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sd7Pgsu6CaXP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yRJ7JU7JCaXT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Grab an image from the test dataset.\n",
    "img = test_images[1]\n",
    "\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vz3bVp21CaXV"
   },
   "source": [
    "`tf.keras` models are optimized to make predictions on a *batch*, or collection, of examples at once. Accordingly, even though you're using a single image, you need to add it to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lDFh5yF_CaXW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Add the image to a batch where it's the only member.\n",
    "img = (np.expand_dims(img,0))\n",
    "\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EQ5wLTkcCaXY"
   },
   "source": [
    "Now predict the correct label for this image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_rzNSdrCaXY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.6265282e-05 4.2805381e-11 9.9573094e-01 1.9248487e-09 1.5190736e-03\n",
      "  3.6040825e-17 2.6936575e-03 1.4191970e-24 1.2967760e-09 1.8266137e-24]]\n"
     ]
    }
   ],
   "source": [
    "predictions_single = model.predict(img)\n",
    "\n",
    "print(predictions_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    predictions_array, true_label = predictions_array, true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(10))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Ai-cpLjO-3A"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_value_array(1, predictions_single[0], test_labels)\n",
    "_ = plt.xticks(range(10), class_names, rotation=45)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classification.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}